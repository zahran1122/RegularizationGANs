{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TQMuUmzaTtRY",
        "kMXKkRL_VMUH",
        "lgxK3XPWTzo0"
      ],
      "authorship_tag": "ABX9TyOvQNr/FTkolnhIrXVoxQ76",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahran1122/RegularizationGANs/blob/main/RegularizationGANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization GANs"
      ],
      "metadata": {
        "id": "TQMuUmzaTtRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run this code, we need to have CelebA sample. First, download the celebA file. Second, create new folder in the colab files and name it \"sample\". Upload all the images from CelebA to the \"sample\" folder. after that, we can run the code."
      ],
      "metadata": {
        "id": "dPbGPsi1UkYD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbRs3azMTklu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, BatchNormalization, Activation, UpSampling2D, Conv2D, Input\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "# Function to load and preprocess real images from the given directory\n",
        "def load_real_samples(directory, image_size=(64, 64)):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            img_path = os.path.join(directory, filename)\n",
        "            img = load_img(img_path, target_size=image_size)\n",
        "            img_array = img_to_array(img)\n",
        "            images.append(img_array)\n",
        "    images = np.array(images, dtype='float32')\n",
        "    images = (images - 127.5) / 127.5\n",
        "    return images\n",
        "\n",
        "# Function to build the generator model\n",
        "def build_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128 * 16 * 16, activation=\"relu\", input_dim=latent_dim))\n",
        "    model.add(Reshape((16, 16, 128)))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(Conv2D(3, kernel_size=4, padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "    return model\n",
        "\n",
        "# Function to build the discriminator model\n",
        "def build_discriminator(image_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=4, strides=2, input_shape=image_shape, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Function to build the GAN model by combining the generator and the discriminator\n",
        "def build_gan(generator, discriminator, latent_dim):\n",
        "    discriminator.trainable = False\n",
        "    gan_input = Input(shape=(latent_dim,))\n",
        "    img = generator(gan_input)\n",
        "    gan_output = discriminator(img)\n",
        "    gan = Model(gan_input, gan_output)\n",
        "    return gan\n",
        "\n",
        "# Function to save the generated images\n",
        "def save_images(generator, epoch, latent_dim, n=5):\n",
        "    noise = np.random.normal(0, 1, (n * n, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(n, n)\n",
        "    cnt = 0\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            axs[i, j].imshow(gen_imgs[cnt])\n",
        "            axs[i, j].axis('off')\n",
        "            cnt += 1\n",
        "    if not os.path.exists('images1'):\n",
        "        os.makedirs('images1')\n",
        "    fig.savefig(f\"images1/gan_images_epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Function to train the GAN model\n",
        "def train_gan_with_mode_seeking(generator, discriminator, gan, dataset, latent_dim, epochs=1000, batch_size=64, save_interval=1, lambda_ms=0.1):\n",
        "    half_batch = int(batch_size / 2)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train the discriminator\n",
        "        idx = np.random.randint(0, dataset.shape[0], half_batch)\n",
        "        real_imgs = dataset[idx]\n",
        "\n",
        "        noise = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "        fake_imgs = generator.predict(noise)\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))\n",
        "        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((half_batch, 1)))\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # Train the generator\n",
        "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "        valid_y = np.array([1] * batch_size)\n",
        "\n",
        "        g_loss = gan.train_on_batch(noise, valid_y)\n",
        "\n",
        "        # Mode-seeking regularization term\n",
        "        z1 = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "        z2 = np.random.normal(0, 1, (half_batch, latent_dim))\n",
        "        g1 = generator.predict(z1)\n",
        "        g2 = generator.predict(z2)\n",
        "\n",
        "        # Compute distances\n",
        "        d_I = np.mean(np.abs(g1 - g2), axis=(1, 2, 3))\n",
        "        d_z = np.mean(np.abs(z1 - z2), axis=1)\n",
        "\n",
        "        # Compute L_ms\n",
        "        L_ms = np.mean(d_I / (d_z + 1e-8))\n",
        "\n",
        "        # Update generator loss with L_ms\n",
        "        g_loss_with_reg = g_loss + lambda_ms * L_ms\n",
        "\n",
        "        # Print the progress\n",
        "        print(f\"{epoch + 1} [D loss: {d_loss[0]}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss_with_reg}] [L_ms: {L_ms}]\")\n",
        "\n",
        "        if (epoch + 1) % save_interval == 0:\n",
        "            save_images(generator, epoch + 1, latent_dim)\n",
        "\n",
        "# Set parameters\n",
        "image_shape = (64, 64, 3)\n",
        "latent_dim = 100\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_real_samples('/content/sample')\n",
        "\n",
        "# Build and compile the models\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(0.0002, 0.5)\n",
        "\n",
        "discriminator = build_discriminator(image_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "generator = build_generator(latent_dim)\n",
        "\n",
        "gan = build_gan(generator, discriminator, latent_dim)\n",
        "gan.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "# Train the GAN model with mode-seeking regularization\n",
        "train_gan_with_mode_seeking(generator, discriminator, gan, dataset, latent_dim, epochs=1000, batch_size=64, save_interval=1, lambda_ms=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convergence Graphs"
      ],
      "metadata": {
        "id": "kMXKkRL_VMUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> CONVERGENCE GRAPHS\n",
        "\n",
        "\n",
        "\n",
        "To implement this code, first run the Regularization GANs code for one time. Copy the output and paste it to notepad name it \"real.text\". Upload the file. Convergence graph will be shown."
      ],
      "metadata": {
        "id": "wXaYGCF_Vdp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is code for Plot of Loss Functions and Regularization term Values**"
      ],
      "metadata": {
        "id": "RcitG_peWNAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reading the data from the file\n",
        "file_path = '/content/real.txt'\n",
        "\n",
        "# Initialize lists to store the data\n",
        "epochs = []\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "l_ms_values = []\n",
        "\n",
        "# Regex pattern to extract the required values\n",
        "pattern = re.compile(r'(\\d+)\\s\\[D loss: ([\\d.]+),.*?\\[G loss: ([\\d.]+)\\].*?\\[L_ms: ([\\d.]+)\\]')\n",
        "\n",
        "# Read and parse the file\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        match = pattern.search(line)\n",
        "        if match:\n",
        "            epoch = int(match.group(1))\n",
        "            d_loss = float(match.group(2))\n",
        "            g_loss = float(match.group(3))\n",
        "            l_ms = float(match.group(4))\n",
        "\n",
        "            epochs.append(epoch)\n",
        "            d_losses.append(d_loss)\n",
        "            g_losses.append(g_loss)\n",
        "            l_ms_values.append(l_ms)\n",
        "\n",
        "# Plotting the combined graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, d_losses, label='D_loss')\n",
        "plt.plot(epochs, g_losses, label='G_loss')\n",
        "# plt.plot(epochs, l_ms_values, label='L_ms')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Convergence Graph')\n",
        "# plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.savefig('/content/convergence_combined.png')\n",
        "plt.show()\n",
        "\n",
        "# L_ms\n",
        "# Plotting the combined graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "# plt.plot(epochs, d_losses, label='D_loss')\n",
        "# plt.plot(epochs, g_losses, label='G_loss')\n",
        "plt.plot(epochs, l_ms_values, label='L_ms')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('L_ms Value')\n",
        "plt.title('Convergence Graph')\n",
        "# plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.savefig('/content/convergence_combined.png')\n",
        "plt.show()\n",
        "\n",
        "# Plotting individual graphs\n",
        "fig, axs = plt.subplots(3, 1, figsize=(10, 18))\n",
        "\n",
        "axs[2].plot(epochs, l_ms_values, color='red')\n",
        "axs[2].set_title('L_ms')\n",
        "axs[2].set_xlabel('Epoch')\n",
        "axs[2].set_ylabel('L_ms Value')\n",
        "plt.tight_layout()\n",
        "# D_loss\n",
        "axs[0].plot(epochs, d_losses, color='blue')\n",
        "axs[0].set_title('D_loss')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "\n",
        "# G_loss\n",
        "axs[1].plot(epochs, g_losses, color='green')\n",
        "axs[1].set_title('G_loss')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('Loss')\n",
        "axs[1].grid(True)"
      ],
      "metadata": {
        "id": "W5Okpr5VWCr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is code for Plots of Discriminator Real and Fake Accuracy**"
      ],
      "metadata": {
        "id": "FI0IwdHXWFiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 读取数据文件\n",
        "file_path = '/content/real.txt'\n",
        "\n",
        "# 初始化存储数据的列表\n",
        "epochs = []\n",
        "real_accuracies = []\n",
        "fake_accuracies = []\n",
        "\n",
        "# 提取所需值的正则表达式模式\n",
        "pattern = re.compile(r'(\\d+)\\s\\[D loss: [\\d.]+, acc.: ([\\d.]+)%\\].*?\\[G loss: [\\d.]+\\].*?\\[L_ms: [\\d.]+\\]')\n",
        "\n",
        "# 读取并解析文件\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        match = pattern.search(line)\n",
        "        if match:\n",
        "            epoch = int(match.group(1))\n",
        "            d_acc = float(match.group(2))\n",
        "\n",
        "            # Assuming real accuracy and fake accuracy are complementary\n",
        "            real_acc = d_acc\n",
        "            fake_acc = 100 - d_acc\n",
        "\n",
        "            epochs.append(epoch)\n",
        "            real_accuracies.append(real_acc)\n",
        "            fake_accuracies.append(fake_acc)\n",
        "\n",
        "# 绘制真实和伪造数据的准确性收敛图\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, real_accuracies, label='Real Accuracy')\n",
        "plt.plot(epochs, fake_accuracies, label='Fake Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Plots of Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/real_fake_accuracy_convergence.png')\n",
        "plt.show()\n",
        "\n",
        "# 显示生成的图像文件路径\n",
        "output_files = {\n",
        "    \"real_fake_accuracy_convergence\": \"/content/real_fake_accuracy_convergence.png\"\n",
        "}\n",
        "\n",
        "output_files\n"
      ],
      "metadata": {
        "id": "LP0Poq7SVQox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FID Score"
      ],
      "metadata": {
        "id": "lgxK3XPWTzo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement this code, first run the Regularization GANs code for one time. Copy the output and paste it to notepad. run this code and upload the run.txt. FID score will be shown."
      ],
      "metadata": {
        "id": "uEAoBQeLULNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Define a function to extract FID scores and epochs\n",
        "def extract_fid_scores(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    # Use regex to find all FID scores and corresponding epochs in the content\n",
        "    matches = re.findall(r'FID score at epoch (\\d+): ([\\d.]+)', content)\n",
        "    epochs = [int(match[0]) for match in matches]\n",
        "    fid_scores = [float(match[1]) for match in matches]\n",
        "\n",
        "    return epochs, fid_scores\n",
        "\n",
        "# Define a function to calculate the mean FID score from a list of scores\n",
        "def calculate_mean_fid(scores):\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Assuming the file is named 'FID-Regularization.txt'\n",
        "file_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Extract FID scores\n",
        "epochs, fid_scores = extract_fid_scores(file_path)\n",
        "\n",
        "# Filter scores for epochs 100 to 1000\n",
        "filtered_epochs = [epoch for epoch in epochs if 100 <= epoch <= 1000]\n",
        "filtered_fid_scores = [fid_scores[i] for i, epoch in enumerate(epochs) if 100 <= epoch <= 1000]\n",
        "\n",
        "# Create a DataFrame to tabulate the values\n",
        "df = pd.DataFrame({\n",
        "    'Epoch': filtered_epochs,\n",
        "    'FID Score': filtered_fid_scores\n",
        "})\n",
        "\n",
        "# Calculate the mean and standard deviation of FID scores\n",
        "mean_fid_score = calculate_mean_fid(filtered_fid_scores)\n",
        "std_fid_score = pd.Series(filtered_fid_scores).std()\n",
        "\n",
        "# Display the table in a prettier format using tabulate\n",
        "print(\"FID Scores from Epoch 100 to 1000:\")\n",
        "print(tabulate(df, headers='keys', tablefmt='pretty'))\n",
        "\n",
        "# Display the mean and standard deviation\n",
        "print(f'\\nMean FID score from epoch 100 to 1000: {mean_fid_score:.2f}')\n",
        "print(f'Standard Deviation of FID score from epoch 100 to 1000: {std_fid_score:.2f}')\n"
      ],
      "metadata": {
        "id": "URoDnXduT4dp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}